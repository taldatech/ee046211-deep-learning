{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "#### Tal Daniel\n",
    "\n",
    "## Tutorial 05 - Multilayer Neural Networks\n",
    "---\n",
    "<img src=\"./assets/mlp.jpg\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Multi-Layer Perceptron (MLP)](#-Multi-Layer-Perceptron-(MLP))\n",
    "* [Modular Approach - Autodiff Reverse Mode](#-Modular-Approach---Autodiff-Reverse-Mode)\n",
    "* [Example - Neural Networks for Regression - Housing Prices](#-Example---Neural-Networks-for-Regression---Housing-Prices)\n",
    "* [Building a Neural Network with PyTorch](#-Building-a-Neural-Network-with-PyTorch)\n",
    "* [Weights Initialization](#-Weights-Initialization)\n",
    "* [Neural Network Weight Initialization with PyTorch](#-Neural-Network-Weight-Initialization-with-PyTorch)\n",
    "* [Deep Double Descent](#-Deep-Double-Descent)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LedcT50PTD1e",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### <img src=\"./assets/colab_icon.PNG\" style=\"height:30px;display:inline\"> Additional Packeges for Google Colab\n",
    "----\n",
    "If you are using <a href=\"https://colab.research.google.com/\">Google Colab</a>, you have to install additional packages. To do this, simply run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O-9ASIdTD1e",
    "outputId": "9e4a777b-5537-47ea-abab-5efd32eed6d1",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# to work locally (win/linux/mac), first install 'graphviz': https://graphviz.org/download/ and restart your machine\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchviz\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/layers.png\" style=\"height:50px;display:inline\"> Multi-Layer Perceptron (MLP)\n",
    "---\n",
    "* An MLP is composed of one input layer, one or more hidden layers and a final output layer. \n",
    "* Every layer, except the output layer includes a **bias neuron** which is fully connected to the next layer. \n",
    "* When the number of hidden layers is larger than 2, the network is usually called a deep neural network (DNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The algorithm is composed of two main parts: **forward pass and backward pass**.\n",
    "* In the *forward pass*, for each training instance, the algorithm feeds it to the network and computes the output of every neuron in each consecutive layer (using the network for prediction is just doing a forward pass). \n",
    "* Then, the output error (the difference between the desired output and the actual output from the network) is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* After the output error calculation, the network calculates how much each neuron in the last hidden layer contributed to the output error (using the **chain rule**).\n",
    "* It then proceeds to measure how much of these error contributions came from each neuron in the previous layers until reaching the input layer. \n",
    "* This is the *backward pass*: measuring the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (this is the backpropagation process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In short: for each training instance the **backpropagation algorithm** first makes a prediction (forward pass), measures the error, then goes in reverse to measure the error contribution from each connection (backward pass) and finally, using Gradient Descent, updates the weights in the direction that reduces the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/mlp_example.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, if: $$ X \\in \\mathbb{R}^2 $$ $$ W_1 \\in \\mathbb{R}^{2 \\times 4} $$ $$ W_2 \\in \\mathbb{R}^{4 \\times 3} $$ $$ W_3 \\in \\mathbb{R}^{3 \\times 1} $$ $$ b_1 \\in \\mathbb{R}^4 $$ $$ b_2 \\in \\mathbb{R}^3 $$ $$ b_3 \\in \\mathbb{R} $$ Then: $$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key change made to the Perceptron that brought upon the era of deep learning is the addition of **activation function** to the output of each neuron. These allow the learning of non-linear functions. We will use three popular activation functions:\n",
    "1. **Logistic function (sigmoid)**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The output is in $[0,1]$ which can be used for binary clssification or as a probability (why?)\n",
    "2. **Hyperbolic tangent function**: $tanh(z) = 2\\sigma(2z) - 1$. The output is in $[-1,1]$ which tends to make each layer's output more or less normalized at the beginning of the training (which may speed up convergence).\n",
    "3. **ReLU (Rectified Linear Unit) function**: $ReLU(z) = max(0,z)$. Continuous but not differentiable at $z=0$. However, it is the most common activation function as it is fast to compute and does not bound the output (which helps with some issues during Gradient Descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/lego-head.png\" style=\"height:50px;display:inline\"> Modular Approach - Autodiff Reverse Mode\n",
    "---\n",
    "* We code **layers**, not networks.\n",
    "* Layer Specification - each layer needs to provide 3 functions:\n",
    "    1. The layer output given its input (forward pass)\n",
    "    2. Derivative with respect to the input\n",
    "    3. Derivative with respect to parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Illustration: <img src=\"./assets/modular_approach_1.jpg\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Zoom-in: <img src=\"./assets/modular_approach_2.jpg\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/plasticine/100/000000/serial-tasks.png\" style=\"height:50px;display:inline\"> Backpropagation\n",
    "---\n",
    "We now establish a common language when it comes to neural networks architecture:\n",
    "* **Forward Pass**: $Z^{(k+1)} = f(Z^{(k)}) $\n",
    "* **Backward Pass**: $\\delta^{(k+1)} = \\frac{\\partial E}{\\partial Z^{(k+1)}}$\n",
    "* Applying the **chain rule** for a single layer: $$ \\frac{\\partial E}{\\partial Z^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial f(Z^{(k)})}{\\partial Z^{(k)}} $$\n",
    "* The **gradient with respect to layer parameters** (if it has any): $$ \\frac{\\partial E}{\\partial W^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = \\delta^{(k+1)} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> Extension to Multi-Dimensions\n",
    "---\n",
    "* $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a vector function of a vector variable: $$ f(x) = \\begin{bmatrix} f_1(x) \\\\ \\vdots \\\\ f_m(x) \\end{bmatrix}, x \\in \\mathbb{R}^n, f(x) \\in \\mathbb{R}^m $$\n",
    "* The **gradient** is given by: $$ \\frac{\\partial f_i}{\\partial x} = \\big[ \\frac{\\partial f_i(x)}{\\partial x_1}, ..., \\frac{\\partial f_i(x)}{\\partial x_n} \\big] $$\n",
    "* The **Jacobian**, $J_f(x) \\in \\mathbb{R}^{m \\times n}$, is given by: $$ J_f(x) = \\begin{bmatrix} \\frac{\\partial f_1(x)}{\\partial x}\\\\ \\vdots\\\\ \\frac{\\partial f_m(x)}{\\partial x} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} && \\cdots && \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots && \\ddots && \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} && \\cdots && \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **The Chain Rule**:\n",
    "    * Given: $$ F: \\mathbb{R}^n \\to \\mathbb{R}^m $$ $$ \\phi: \\mathbb{R}^m \\to \\mathbb{R}^k $$ $$ \\psi(x)= \\phi(F(x)) $$\n",
    "    * The Jacobian is given by: $$ J_{\\psi} = J_{\\phi} J_F $$ $$ J_{\\phi} \\in \\mathbb{R}^{k \\times m}, J_F \\in  \\mathbb{R}^{m \\times n} \\to J_{\\psi} \\in \\mathbb{R}^{k \\times n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/backprop_anim.gif\" style=\"height:350px\">\n",
    "\n",
    "<a href=\"https://anatomiesofintelligence.github.io/posts/2018-10-16-forward-back-propogation\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/popular-topic.png\" style=\"height:50px;display:inline\"> Commonly Used Layers (as Modular Blocks)\n",
    "---\n",
    "* Linear Layer (linear combination of the inputs)\n",
    "* Activation Layer (usually together with the linear layer, apply a function on the linear combination of weighted inputs): ReLU, Binary Step, Sigmoid, TanH...\n",
    "* Softmax Layer (Sigmoid for for than 2 classes, outputs the probability of each class)\n",
    "* Loss Function Layer (MSE, Cross Entropy...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/home.png\" style=\"height:50px;display:inline\"> Example - Neural Networks for Regression - Housing Prices\n",
    "---\n",
    "* The Housing Prices Dataset:\n",
    "    * Two input features: *Size* and *Floor*\n",
    "    * One output: *House Price*\n",
    "    * **Loss function**: MSE\n",
    "* Suggested **network architecture**: 2 hidden layers\n",
    "    * Two inputs, one for each feature\n",
    "    * Four neurons in the *first hidden layer*\n",
    "    * Three neurons in the *second hidden layer*\n",
    "    * One output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Layout: <img src=\"./assets/mlp_example.jpg\" style=\"height:300px\"> $$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where: $$ X \\in \\mathbb{R}^2 $$ $$ W_1 \\in \\mathbb{R}^{2 \\times 4} $$ $$ W_2 \\in \\mathbb{R}^{4 \\times 3} $$ $$ W_3 \\in \\mathbb{R}^{3 \\times 1} $$ $$ b_1 \\in \\mathbb{R}^4 $$ $$ b_2 \\in \\mathbb{R}^3 $$ $$ b_3 \\in \\mathbb{R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/office/80/000000/baby-footprints-path.png\" style=\"height:50px;display:inline\"> Step-by-Step Solution\n",
    "---\n",
    "* The MSE loss function over all the training examples $x_i$ and the corresponding training targets: $$ Error = \\frac{1}{N} \\sum_{i=1}^N (F(x_i, W) - y_i)^2 = \\frac{1}{N} ||F(X, W) - Y||_2^2 $$\n",
    "* Linear Layer: $$ u_{out} = W^Tu_{in} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Activation Layer:\n",
    "    * $\\phi_1$ and $\\phi_2$ are multivariate vector *nonlinear* functions, such that: $$ \\phi(U) = \\phi\\left(\\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix}\\right) = \\begin{bmatrix} \\phi(u_1) \\\\ \\vdots \\\\ \\phi(u_n) \\end{bmatrix} $$\n",
    "    * For **ReLU**: $$ \\begin{bmatrix} \\phi(u_1) \\\\ \\vdots \\\\ \\phi(u_n) \\end{bmatrix} = \\begin{bmatrix} \\max(0, u_1) \\\\ \\vdots \\\\ \\max(0, u_n) \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\"> The Linear Layer\n",
    "---\n",
    "* **Forward Pass**: $$Z^{(k+1)} = f(Z^{(k)}) = (W^{(k)})^T Z^{(k)} + b^{(k)}$$\n",
    "    * $k$ denotes the $k^{th}$ layer with the corresponding weights and bias $W^{(k)}, b^{(k)}$\n",
    "* **Derivative** with respect to *input* $Z^{(k)}$: $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\frac{\\partial ((W^{(k)})^T Z^{(k)} + b^{(k)})}{\\partial Z^{(k)}} = (W^{(k)})^T $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} (W^{(k)})^T  $$\n",
    "* **Derivative** with respect to the *parameters* $W^{(k)}, b^{(k)}$: $$ \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = Z^{(k)} ,\\frac{\\partial E}{\\partial W^{(k)}} = \\delta^{(k+1)} Z^{(k)} $$ <br> $$  \\frac{\\partial Z^{(k+1)}}{\\partial b^{(k)}} = I,  \\frac{\\partial E}{\\partial b^{(k)}} = \\delta^{(k+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\">  The ReLU Layer\n",
    "---\n",
    "* **Forward Pass**: $$ Z^{(k+1)} = \\begin{bmatrix} max(0,Z_1^{(k)}) \\\\ \\vdots \\\\ max(0,Z_n^{(k)}) \\end{bmatrix}, ReLU(Z): \\mathbb{R}^{n} \\to \\mathbb{R}^n $$\n",
    "* **Derivative** with respect to *input* $Z^{(k)}$: $$ \\phi = max(0,Z^{(k)}), \\phi' = heaviside(Z^{(k)}) $$ <br> $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = diag(\\phi') $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} diag (\\phi') $$\n",
    "* **Derivative** with respect to the *parameters*: **NO PARAMETERS!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:20px;display:inline\">  The MSE Layer \n",
    "---\n",
    "* **Forward Pass**: $$ E = Z^{(k+1)} = (Z^{(k)} - y)^2 $$\n",
    "*  **Derivative** with respect to *input* $Z^{(k)}$: $$ \\delta^{(k+1)} = \\frac{\\partial E}{\\partial Z^{(k+1)}} = \\frac{\\partial E}{\\partial E} = 1 $$ <br> $$ \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = 2(Z^{(k)} - y) $$ <br> $$ \\delta^{(k)} = \\delta^{(k+1)} 2(Z^{(k)} - y) =  2(Z^{(k)} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/fast-forward.png\" style=\"height:50px;display:inline\"> Forward Pass\n",
    "---\n",
    "$$ F(X,W) = W_3^T \\phi_2(W_2^T\\phi_1(W_1^TX + b_1) + b_2) + b_3 $$\n",
    "<img src=\"./assets/forward_pass.JPG\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/rewind.png\" style=\"height:50px;display:inline\"> Backward Pass\n",
    "---\n",
    "<img src=\"./assets/backward_pass.JPG\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Building a Neural Network with PyTorch\n",
    "---\n",
    "We will now implement a neural network for regression with PyTorch. We will use the \"Boston House Prices\" dataset and use the architecture described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define our neural network model\n",
    "class HousePricesMLP(nn.Module):\n",
    "    # notice that we inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        # here we initialize the building blocks of our network\n",
    "        # single neuron is just one linear (fully-connected) layer\n",
    "        self.fc_1 = nn.Linear(input_dim, 4) \n",
    "        self.fc_2 = nn.Linear(4, 3)\n",
    "        self.output_layer = nn.Linear(3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we define what happens to the input x in the forward pass\n",
    "        # that is, the order in which x goes through the building blocks\n",
    "        # in our case, x first goes through the signle neuron and then activated with sigmoid\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = torch.relu(self.fc_2(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# alternative method\n",
    "class HousePricesMLP(nn.Module):\n",
    "    # notice that we inherit from nn.Module\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HousePricesMLP, self).__init__()\n",
    "        # here we initialize the building blocks of our network\n",
    "        # single neuron is just one linear (fully-connected) layer\n",
    "        self.hidden = nn.Sequential(nn.Linear(input_dim, 4),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(4, 3),\n",
    "                                    nn.ReLU())\n",
    "        self.output_layer = nn.Linear(3, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # here we define what happens to the input x in the forward pass\n",
    "        # that is, the order in which x goes through the building blocks\n",
    "        # in our case, x first goes through the signle neuron and then activated with sigmoid\n",
    "        return self.output_layer(self.hidden(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data and preprocess\n",
    "boston_dataset = load_boston()\n",
    "# print description of the features\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>12.04820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.648</td>\n",
       "      <td>87.6</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>291.55</td>\n",
       "      <td>14.10</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.05372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>6.549</td>\n",
       "      <td>51.0</td>\n",
       "      <td>5.9604</td>\n",
       "      <td>4.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>392.85</td>\n",
       "      <td>7.39</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.07950</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>6.579</td>\n",
       "      <td>35.9</td>\n",
       "      <td>10.7103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>370.78</td>\n",
       "      <td>5.49</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.53412</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647</td>\n",
       "      <td>7.520</td>\n",
       "      <td>89.4</td>\n",
       "      <td>2.1398</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>388.37</td>\n",
       "      <td>7.26</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.01870</td>\n",
       "      <td>85.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429</td>\n",
       "      <td>6.516</td>\n",
       "      <td>27.7</td>\n",
       "      <td>8.5353</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>392.43</td>\n",
       "      <td>6.36</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.546</td>\n",
       "      <td>33.1</td>\n",
       "      <td>3.1323</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>390.96</td>\n",
       "      <td>5.33</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>10.83420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>6.782</td>\n",
       "      <td>90.8</td>\n",
       "      <td>1.8195</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.57</td>\n",
       "      <td>25.79</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE      DIS   RAD    TAX  \\\n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377  94.3   6.3467   5.0  311.0   \n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5   2.8617   8.0  307.0   \n",
       "422  12.04820   0.0  18.10   0.0  0.614  5.648  87.6   1.9512  24.0  666.0   \n",
       "296   0.05372   0.0  13.92   0.0  0.437  6.549  51.0   5.9604   4.0  289.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421  78.9   4.9671   2.0  242.0   \n",
       "351   0.07950  60.0   1.69   0.0  0.411  6.579  35.9  10.7103   4.0  411.0   \n",
       "261   0.53412  20.0   3.97   0.0  0.647  7.520  89.4   2.1398   5.0  264.0   \n",
       "347   0.01870  85.0   4.15   0.0  0.429  6.516  27.7   8.5353   4.0  351.0   \n",
       "175   0.06664   0.0   4.05   0.0  0.510  6.546  33.1   3.1323   5.0  296.0   \n",
       "416  10.83420   0.0  18.10   0.0  0.679  6.782  90.8   1.8195  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "10      15.2  392.52  20.45  15.0  \n",
       "220     17.4  391.70   9.71  26.7  \n",
       "422     20.2  291.55  14.10  20.8  \n",
       "296     16.0  392.85   7.39  27.1  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "351     18.3  370.78   5.49  24.1  \n",
       "261     13.0  388.37   7.26  43.1  \n",
       "347     17.9  392.43   6.36  23.1  \n",
       "175     16.6  390.96   5.33  29.4  \n",
       "416     20.2   21.57  25.79   7.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target is the MEDV field - median value of owner-occupied homes in 1000$\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston['MEDV'] = boston_dataset.target\n",
    "boston.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training samples: 404, total test samples: 102\n"
     ]
    }
   ],
   "source": [
    "# we will use 2 features\n",
    "x = boston[['RM', 'LSTAT']].values  # RM-num rooms, LSTAT-% lower status of the population\n",
    "y = boston['MEDV'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "# scaling\n",
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(x_train)\n",
    "x_train = x_scaler.transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "print(\"total training samples: {}, total test samples: {}\".format(len(x_train),len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: features: tensor([-0.8488,  0.8353]), target: 13.100000381469727\n"
     ]
    }
   ],
   "source": [
    "# convert to tensor dataset for PyTorch\n",
    "boston_tensor_train_ds = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "# check\n",
    "print(f'sample 0: features: {boston_tensor_train_ds[0][0]}, target: {boston_tensor_train_ds[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define hyper-parmeters and create our model\n",
    "num_features = 2\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "# model\n",
    "model = HousePricesMLP(num_features, output_dim).to(device)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 611.3372344970703\n",
      "epoch: 50 loss: 26.573792934417725\n",
      "epoch: 100 loss: 23.42008924484253\n",
      "epoch: 150 loss: 22.77083158493042\n",
      "epoch: 200 loss: 22.498516082763672\n",
      "epoch: 250 loss: 22.412588596343994\n",
      "epoch: 300 loss: 22.351622104644775\n",
      "epoch: 350 loss: 22.29261350631714\n",
      "epoch: 400 loss: 22.230255603790283\n",
      "epoch: 450 loss: 22.17220449447632\n"
     ]
    }
   ],
   "source": [
    "boston_tensor_train_dataloader = DataLoader(boston_tensor_train_ds, batch_size=batch_size)\n",
    "\n",
    "# training loop for the model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for features, targets in boston_tensor_train_dataloader:\n",
    "        # send data to device\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # forward pass\n",
    "        output = model(features)\n",
    "        # loss\n",
    "        loss = criterion(output.view(-1), targets)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # clean the gradients from previous iteration\n",
    "        loss.backward()  # autograd backward to calculate gradients\n",
    "        optimizer.step()  # apply update to the weights\n",
    "        epoch_losses.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE error: 15.394033432006836\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.from_numpy(x_test).float().to(device))\n",
    "    test_error = criterion(test_outputs.view(-1), torch.from_numpy(y_test).float().to(device))\n",
    "print(f'test MSE error: {test_error.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\r\n",
       " -->\r\n",
       "<!-- Pages: 1 -->\r\n",
       "<svg width=\"374pt\" height=\"371pt\"\r\n",
       " viewBox=\"0.00 0.00 374.00 371.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 367)\">\r\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-367 370,-367 370,4 -4,4\"/>\r\n",
       "<!-- 2419740177632 -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>2419740177632</title>\r\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"223.5,-21 122.5,-21 122.5,0 223.5,0 223.5,-21\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-7.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180208 -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>2419740180208</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"110,-78 0,-78 0,-57 110,-57 110,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">output_layer.bias (1)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180208&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>2419740180208&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.55,-56.92C94.22,-48.22 121.98,-35.28 143.1,-25.43\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.7,-28.55 152.29,-21.16 141.74,-22.21 144.7,-28.55\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740177688 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>2419740177688</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-78 128,-78 128,-57 218,-57 218,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">ReluBackward0</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740177688&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>2419740177688&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M173,-56.92C173,-49.91 173,-40.14 173,-31.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.5,-31.34 173,-21.34 169.5,-31.34 176.5,-31.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740178304 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>2419740178304</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213.5,-135 112.5,-135 112.5,-114 213.5,-114 213.5,-135\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-121.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740178304&#45;&gt;2419740177688 -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>2419740178304&#45;&gt;2419740177688</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.74,-113.92C166.02,-106.91 167.79,-97.14 169.37,-88.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"172.87,-88.81 171.21,-78.34 165.98,-87.55 172.87,-88.81\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180152 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>2419740180152</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"99.5,-192 6.5,-192 6.5,-171 99.5,-171 99.5,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.2.bias (3)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180152&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>2419740180152&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.16,-170.92C89.41,-162.3 114.98,-149.51 134.6,-139.7\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.31,-142.76 143.69,-135.16 133.18,-136.5 136.31,-142.76\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740178360 -->\r\n",
       "<g id=\"node6\" class=\"node\">\r\n",
       "<title>2419740178360</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-192 118,-192 118,-171 208,-171 208,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">ReluBackward0</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740178360&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge5\" class=\"edge\">\r\n",
       "<title>2419740178360&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163,-170.92C163,-163.91 163,-154.14 163,-145.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"166.5,-145.34 163,-135.34 159.5,-145.34 166.5,-145.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179816 -->\r\n",
       "<g id=\"node7\" class=\"node\">\r\n",
       "<title>2419740179816</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206.5,-249 105.5,-249 105.5,-228 206.5,-228 206.5,-249\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-235.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">AddmmBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179816&#45;&gt;2419740178360 -->\r\n",
       "<g id=\"edge6\" class=\"edge\">\r\n",
       "<title>2419740179816&#45;&gt;2419740178360</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.22,-227.92C158.11,-220.91 159.35,-211.14 160.46,-202.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.96,-202.7 161.75,-192.34 157.01,-201.82 163.96,-202.7\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179648 -->\r\n",
       "<g id=\"node8\" class=\"node\">\r\n",
       "<title>2419740179648</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"152.5,-306 59.5,-306 59.5,-285 152.5,-285 152.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-292.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.0.bias (4)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179648&#45;&gt;2419740179816 -->\r\n",
       "<g id=\"edge7\" class=\"edge\">\r\n",
       "<title>2419740179648&#45;&gt;2419740179816</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.71,-284.92C121.72,-277.21 131.76,-266.16 140.18,-256.9\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.92,-259.09 147.05,-249.34 137.74,-254.39 142.92,-259.09\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179760 -->\r\n",
       "<g id=\"node9\" class=\"node\">\r\n",
       "<title>2419740179760</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"241.5,-306 170.5,-306 170.5,-285 241.5,-285 241.5,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-292.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179760&#45;&gt;2419740179816 -->\r\n",
       "<g id=\"edge8\" class=\"edge\">\r\n",
       "<title>2419740179760&#45;&gt;2419740179816</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M197.29,-284.92C190.28,-277.21 180.24,-266.16 171.82,-256.9\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.26,-254.39 164.95,-249.34 169.08,-259.09 174.26,-254.39\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180096 -->\r\n",
       "<g id=\"node10\" class=\"node\">\r\n",
       "<title>2419740180096</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"264,-363 148,-363 148,-342 264,-342 264,-363\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-349.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.0.weight (4, 2)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180096&#45;&gt;2419740179760 -->\r\n",
       "<g id=\"edge9\" class=\"edge\">\r\n",
       "<title>2419740180096&#45;&gt;2419740179760</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206,-341.92C206,-334.91 206,-325.14 206,-316.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"209.5,-316.34 206,-306.34 202.5,-316.34 209.5,-316.34\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180320 -->\r\n",
       "<g id=\"node11\" class=\"node\">\r\n",
       "<title>2419740180320</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"311.5,-192 240.5,-192 240.5,-171 311.5,-171 311.5,-192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"276\" y=\"-178.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180320&#45;&gt;2419740178304 -->\r\n",
       "<g id=\"edge10\" class=\"edge\">\r\n",
       "<title>2419740180320&#45;&gt;2419740178304</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.32,-170.92C238.52,-162.26 212.09,-149.4 191.9,-139.57\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.36,-136.38 182.84,-135.16 190.3,-142.68 193.36,-136.38\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740179928 -->\r\n",
       "<g id=\"node12\" class=\"node\">\r\n",
       "<title>2419740179928</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"341,-249 225,-249 225,-228 341,-228 341,-249\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"283\" y=\"-235.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">hidden.2.weight (3, 4)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740179928&#45;&gt;2419740180320 -->\r\n",
       "<g id=\"edge11\" class=\"edge\">\r\n",
       "<title>2419740179928&#45;&gt;2419740180320</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.78,-227.92C280.89,-220.91 279.65,-211.14 278.54,-202.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.99,-201.82 277.25,-192.34 275.04,-202.7 281.99,-201.82\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740180264 -->\r\n",
       "<g id=\"node13\" class=\"node\">\r\n",
       "<title>2419740180264</title>\r\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"325.5,-78 254.5,-78 254.5,-57 325.5,-57 325.5,-78\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"290\" y=\"-64.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">TBackward</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740180264&#45;&gt;2419740177632 -->\r\n",
       "<g id=\"edge12\" class=\"edge\">\r\n",
       "<title>2419740180264&#45;&gt;2419740177632</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.62,-56.92C251.11,-48.22 223.58,-35.28 202.64,-25.43\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.08,-22.24 193.54,-21.16 201.1,-28.58 204.08,-22.24\"/>\r\n",
       "</g>\r\n",
       "<!-- 2419740176568 -->\r\n",
       "<g id=\"node14\" class=\"node\">\r\n",
       "<title>2419740176568</title>\r\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"366,-135 232,-135 232,-114 366,-114 366,-135\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"299\" y=\"-121.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">output_layer.weight (1, 3)</text>\r\n",
       "</g>\r\n",
       "<!-- 2419740176568&#45;&gt;2419740180264 -->\r\n",
       "<g id=\"edge13\" class=\"edge\">\r\n",
       "<title>2419740176568&#45;&gt;2419740180264</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.43,-113.92C296.28,-106.91 294.69,-97.14 293.27,-88.47\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296.68,-87.64 291.61,-78.34 289.77,-88.77 296.68,-87.64\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x23363c0ff98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize computational graph\n",
    "x = torch.randn(1, num_features).to(device)\n",
    "torchviz.make_dot(model(x), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/re-enter-pincode.png\" style=\"height:50px;display:inline\"> Weights Initialization\n",
    "---\n",
    "* As we have learned, neural networks are trained using a stochastic optimization algorithm, such as Gradient Descent, RMSprop, Adam and etc...\n",
    "* Recall that these algorithms require initializing the parameters to some values. That is, they use randomness in order to find a good enough set of weights for the specific mapping function from inputs to outputs in your data that is being learned.\n",
    "* These algroithms require that the weights of the network are initialized to small random values (random, but close to zero). \n",
    "    * Randomness is also used during the search process in the **shuffling of the training dataset** prior to each epoch, which in turn results in differences in the gradient estimate for each batch.\n",
    "* Training deep models is a sufficiently difficult task that most algorithms are strongly affected by the choice of initialization (page 301, <a href=\"https://amzn.to/2H5wjfg\">Deep Learning</a>, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why Not Just Initialize With Zeros?\n",
    "---\n",
    "* We can use the same set of weights each time we train the network. For example, you could use the values of 0.0 for all weights.\n",
    "* In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be **stuck**. \n",
    "    * It is important to note that the bias weight in each neuron is set to zero by default, not a small random value.\n",
    "* Specifically, neurons that are in the same hidden layer that is connected to the same inputs must have different weights for the learning algorithm to update the weights.\n",
    "* **Symmetry Breaking**: initial parameters need to break symmetry between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters (page 301, <a href=\"https://amzn.to/2H5wjfg\">Deep Learning</a>, 2016). \n",
    "    * Why? If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.\n",
    "* Note that when you **constant the seed**, you will initialize with the same weight seach time. We do this when we want to get reproducible results (or in production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/emoji/96/000000/on-arrow-emoji.png\" style=\"height:50px;display:inline\"> Types of Weight Initialization\n",
    "---\n",
    "* The initialization of the weights of neural networks is an active field of study as the careful initialization of the network can speed up the learning process.\n",
    "\n",
    "* There is no single best way to initialize the weights of a neural network.\n",
    "\n",
    "* We will review some of the popular initalization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Unifrom** - initialize with values drawn from the uniform distribution $\\mathcal{U}(a, b)$\n",
    "    * In PyTorch - `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`\n",
    "* **Normal** - initialize with values drawn from the normal distribution $\\mathcal{N}(\\text{mean}, \\text{std}^2)$\n",
    "    * In PyTorch - `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`\n",
    "* **Constant** - initialize with the value $val$.\n",
    "    * In PyTorch - `torch.nn.init.constant_(tensor, val)`\n",
    "* **Ones** - Initialize with the scalar value 1.\n",
    "    * In PyTorch - `torch.nn.init.ones_(tensor)`\n",
    "* **Zeros** - Initialize with the scalar value 0.\n",
    "    * In PyTorch - `torch.nn.init.zeros_(tensor)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Xavier (Glorot) Uniform** - Initialize with values according to the method described in *Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)*, using a uniform distribution. The resulting tensor will have values sampled from $\\mathcal{U}(-a, a)$ where $$ a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan}_{in} + \\text{fan}_{out}}} $$\n",
    "    * `fan_in` is the number of input units in the weight tensor and `fan_out` is the number of output units in the weight tensor\n",
    "    * In PyTorch - `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`\n",
    "    \n",
    "* **Xavier (Glorot) Normal** - Initialize with values according to the method described in *Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)*, using a normal distribution. The resulting tensor will have values sampled from $\\mathcal{N}(0,\\text{std}^2)$ where $$ \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan}_{in} + \\text{fan}_{out}}} $$\n",
    "    * `fan_in` is the number of input units in the weight tensor and `fan_out` is the number of output units in the weight tensor\n",
    "    * In PyTorch - `torch.nn.init.xavier_normal_(tensor, gain=1.0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Kaiming (He) Uniform** - Initialize with values according to the method described in *Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015)*, using a uniform distribution. The resulting tensor will have values sampled from $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where $$ \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan_mode}}} $$\n",
    "    * In PyTorch - `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`\n",
    "    * `a` - the negative slope of the rectifier used after this layer (only used with `leaky_relu`)\n",
    "    * `fan_mode`  either `fan_in` (default) or `fan_out`. Choosing `fan_in` preserves the magnitude of the variance of the weights in the forward pass. Choosing `fan_out` preserves the magnitudes in the backwards pass.\n",
    "    * `nonlinearity`  the non-linear function (`nn.functional` name), recommended to use only with `relu` or `leaky_relu` (default).\n",
    "    \n",
    "* **Kaiming (He) Normal** - Initialize with values according to the method described in  *Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015)*, using a normal distribution. The resulting tensor will have values sampled from $\\mathcal{N}(0,\\text{std}^2)$ where $$ \\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan_mode}}} $$\n",
    "    * In PyTorch - `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Interactive Demo\n",
    "---\n",
    "<a href=\"https://www.deeplearning.ai/ai-notes/initialization/\">Different Initializations Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Neural Network Weight Initialization with PyTorch\n",
    "---\n",
    "* As from PyTorch 1.0, **most layers are initialized using Kaiming Uniform method by default**.\n",
    "* Let's see how we change the initialization of a model.\n",
    "* <a href=\"https://pytorch.org/docs/stable/nn.init.html\">Official PyTorch initialization documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define hyper-parmeters and create our model\n",
    "num_features = 2\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "# model\n",
    "model = HousePricesMLP(num_features, output_dim).to(device)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HousePricesMLP(\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a different initialization for the model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 624.5885009765625\n",
      "epoch: 50 loss: 35.445608139038086\n",
      "epoch: 100 loss: 28.66716480255127\n",
      "epoch: 150 loss: 22.615984439849854\n",
      "epoch: 200 loss: 20.051589488983154\n",
      "epoch: 250 loss: 19.82143211364746\n",
      "epoch: 300 loss: 19.730319499969482\n",
      "epoch: 350 loss: 19.615483283996582\n",
      "epoch: 400 loss: 19.475394248962402\n",
      "epoch: 450 loss: 19.32789421081543\n",
      "test MSE error: 15.348824501037598\n"
     ]
    }
   ],
   "source": [
    "boston_tensor_train_dataloader = DataLoader(boston_tensor_train_ds, batch_size=batch_size)\n",
    "\n",
    "# training loop for the model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for features, targets in boston_tensor_train_dataloader:\n",
    "        # send data to device\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # forward pass\n",
    "        output = model(features)\n",
    "        # loss\n",
    "        loss = criterion(output.view(-1), targets)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # clean the gradients from previous iteration\n",
    "        loss.backward()  # autograd backward to calculate gradients\n",
    "        optimizer.step()  # apply update to the weights\n",
    "        epoch_losses.append(loss.item())\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')\n",
    "        \n",
    "# test error\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(torch.from_numpy(x_test).float().to(device))\n",
    "    test_error = criterion(test_outputs.view(-1), torch.from_numpy(y_test).float().to(device))\n",
    "print(f'test MSE error: {test_error.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/alps.png\" style=\"height:50px;display:inline\"> Deep Double Descent\n",
    "---\n",
    "* Double Descent in ML algorithms training: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time.\n",
    "* This effect is often avoided through careful **regularization** or **early stopping**. \n",
    "    * While this behavior appears to be fairly universal, *we dont yet fully understand why it happens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/double_descent_1.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* It can be seen that as we increase the number of parameters in a model, the test error initially decreases, increases, and, just as the model is able to fit the train set, undergoes a second descent. This different than what we saw when we talked about the bias-variance trade-off.\n",
    "* Double descent also occurs over **train epochs**. \n",
    "    * Surprisingly, it can lead to a regime where **more data hurts**, and training a deep network on a larger train set actually performs worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model-wise Double Descent\n",
    "---\n",
    "* There is a regime where **bigger models are worse**.\n",
    "* The model-wise double descent phenomenon can lead to a regime where training on more data hurts.\n",
    "\n",
    "<img src=\"./assets/double_descent_model.svg\" style=\"height:300px\">\n",
    "\n",
    "In the figure, the peak in test error occurs around the interpolation threshold, **when the models are just barely large enough to fit the train set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sample-wise Non-monotonicity\n",
    "---\n",
    "* There is a regime where **more samples hurts**.\n",
    "\n",
    "<img src=\"./assets/double_descent_sample.svg\" style=\"height:300px\">\n",
    "\n",
    "* In the figure, increasing the number of samples shifts the curve downwards towards lower test error. \n",
    "* However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right.\n",
    "* For intermediate model sizes (red arrows), these two effects combine, and training on 4.5x more samples actually hurts test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Epoch-wise Double Descent\n",
    "---\n",
    "* There is a regime where **training longer reverses overfitting**.\n",
    "\n",
    "<img src=\"./assets/double_descent_epoch.png\" style=\"height:300px\">\n",
    "<img src=\"./assets/double_descent_epoch_2.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The figures above show test and train error as a function of both model size and number of optimization steps.\n",
    "* For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent.\n",
    "* For a given model size (fixed x-coordinate), as training proceeds, **test and train error decreases, increases, and decreases again!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**In general, the peak of test error appears systematically when models are just barely able to fit the train set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* Deep Learning - <a href=\"https://www.youtube.com/watch?v=kPXxbmBsFxs\">Machine Learning Lecture 35 \"Neural Networks / Deep Learning\" -Cornell CS4780</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=zmu9wR2c7Z4\">Machine Learning Lecture 36 \"Neural Networks / Deep Learning Continued\" -Cornell CS4780</a>\n",
    "* Building a Network with PyTorch - <a href=\"https://www.youtube.com/watch?v=ixathu7U-LQ\">Deep Learning and Neural Networks with Python and Pytorch</a>\n",
    "* Weight Initialization - <a href=\"https://www.youtube.com/watch?v=m1gt7nxbB2k\">UC Berkeley - STAT 157- Stabilize Training - Weight Initialization</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=tMjdQLylyGI\">Krish Naik - Various Weight Initialization Techniques in Neural Network</a>\n",
    "    * <a href=\"https://www.youtube.com/watch?v=s2coXdufOzE\">Weight Initialization in a Deep Network (C2W1L11)</a>\n",
    "* Deep Double Descent - <a href=\"https://www.youtube.com/watch?v=R29awq6jvUw\">Henry AI Labs - Deep Double Descent</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* <a href=\"https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\">Jason Brownlee - Why Initialize a Neural Network with Random Weights?</a>\n",
    "* <a href=\"https://openai.com/blog/deep-double-descent/\">OpenAI - Deep Double Descent</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
